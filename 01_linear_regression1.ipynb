{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garyditlow/anaconda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = np.linspace(-1, 1, 101)\n",
    "trY = 2 * trX + np.random.randn(*trX.shape) * 0.33 # create a y value which is approximately linear but with some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = np.array([38163, 80472, 18606, 18239, 155480],dtype=float)\n",
    "trY = np.array([39470, 80336, 21923, 20331, 162060],dtype=float)\n",
    "\n",
    "trX/= 100000\n",
    "trY/= 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\") # create symbolic variables\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "def model(X, w, b):\n",
    "    return tf.multiply(X, w) + b # lr is just X*w so this model line is pretty simple\n",
    "\n",
    "w = tf.Variable(1.0, name=\"w\")\n",
    "b = tf.Variable(1.0, name=\"b\")\n",
    "# create a shared variable (like theano.shared) for the weight matrix\n",
    "y_model = model(X, w, b)\n",
    "\n",
    "cost = tf.square(Y - y_model) # use square error for cost function\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost) # construct an optimizer to minimize cost and fit line to my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.992467\n",
      "0 0.976766\n",
      "0 0.97333\n",
      "0 0.969987\n",
      "0 0.944753\n",
      "1 0.938086\n",
      "1 0.924537\n",
      "1 0.921463\n",
      "1 0.918466\n",
      "1 0.898319\n",
      "2 0.892399\n",
      "2 0.880702\n",
      "2 0.877941\n",
      "2 0.875243\n",
      "2 0.859445\n",
      "3 0.854171\n",
      "3 0.844066\n",
      "3 0.841576\n",
      "3 0.839139\n",
      "3 0.827054\n",
      "4 0.82234\n",
      "4 0.813603\n",
      "4 0.811349\n",
      "4 0.809138\n",
      "4 0.800221\n",
      "5 0.79599\n",
      "5 0.788431\n",
      "5 0.786382\n",
      "5 0.784368\n",
      "5 0.778149\n",
      "6 0.774338\n",
      "6 0.76779\n",
      "6 0.76592\n",
      "6 0.764077\n",
      "6 0.760154\n",
      "7 0.756707\n",
      "7 0.751029\n",
      "7 0.749314\n",
      "7 0.747621\n",
      "7 0.745647\n",
      "8 0.742516\n",
      "8 0.737585\n",
      "8 0.736007\n",
      "8 0.734444\n",
      "8 0.734123\n",
      "9 0.731267\n",
      "9 0.726979\n",
      "9 0.72552\n",
      "9 0.724071\n",
      "9 0.725147\n",
      "10 0.72253\n",
      "10 0.718794\n",
      "10 0.71744\n",
      "10 0.716091\n",
      "10 0.718346\n",
      "11 0.715937\n",
      "11 0.712676\n",
      "11 0.711413\n",
      "11 0.710153\n",
      "11 0.713398\n",
      "12 0.711171\n",
      "12 0.708317\n",
      "12 0.707136\n",
      "12 0.705954\n",
      "12 0.710027\n",
      "13 0.707959\n",
      "13 0.705456\n",
      "13 0.704347\n",
      "13 0.703233\n",
      "13 0.707997\n",
      "14 0.706069\n",
      "14 0.703867\n",
      "14 0.702821\n",
      "14 0.701768\n",
      "14 0.707104\n",
      "15 0.705299\n",
      "15 0.703355\n",
      "15 0.702365\n",
      "15 0.701367\n",
      "15 0.707173\n",
      "16 0.705477\n",
      "16 0.703754\n",
      "16 0.702815\n",
      "16 0.701865\n",
      "16 0.708055\n",
      "17 0.706454\n",
      "17 0.704921\n",
      "17 0.704028\n",
      "17 0.703121\n",
      "17 0.70962\n",
      "18 0.708104\n",
      "18 0.706734\n",
      "18 0.705882\n",
      "18 0.705014\n",
      "18 0.711758\n",
      "19 0.710317\n",
      "19 0.709088\n",
      "19 0.708272\n",
      "19 0.70744\n",
      "19 0.714375\n",
      "20 0.713001\n",
      "20 0.711892\n",
      "20 0.711109\n",
      "20 0.710309\n",
      "20 0.717388\n",
      "21 0.716074\n",
      "21 0.715068\n",
      "21 0.714315\n",
      "21 0.713545\n",
      "21 0.720729\n",
      "22 0.719469\n",
      "22 0.718551\n",
      "22 0.717826\n",
      "22 0.717082\n",
      "22 0.724338\n",
      "23 0.723127\n",
      "23 0.722284\n",
      "23 0.721584\n",
      "23 0.720864\n",
      "23 0.728164\n",
      "24 0.726996\n",
      "24 0.726218\n",
      "24 0.725542\n",
      "24 0.724845\n",
      "24 0.732162\n",
      "25 0.731035\n",
      "25 0.730311\n",
      "25 0.729657\n",
      "25 0.728981\n",
      "25 0.736297\n",
      "26 0.735206\n",
      "26 0.73453\n",
      "26 0.733896\n",
      "26 0.733239\n",
      "26 0.740535\n",
      "27 0.739478\n",
      "27 0.738842\n",
      "27 0.738227\n",
      "27 0.737588\n",
      "27 0.744851\n",
      "28 0.743825\n",
      "28 0.743222\n",
      "28 0.742625\n",
      "28 0.742003\n",
      "28 0.74922\n",
      "29 0.748222\n",
      "29 0.747649\n",
      "29 0.747068\n",
      "29 0.746463\n",
      "29 0.753623\n",
      "30 0.752652\n",
      "30 0.752103\n",
      "30 0.751539\n",
      "30 0.750948\n",
      "30 0.758044\n",
      "31 0.757098\n",
      "31 0.75657\n",
      "31 0.75602\n",
      "31 0.755444\n",
      "31 0.762468\n",
      "32 0.761545\n",
      "32 0.761035\n",
      "32 0.760499\n",
      "32 0.759937\n",
      "32 0.766884\n",
      "33 0.765982\n",
      "33 0.765487\n",
      "33 0.764964\n",
      "33 0.764415\n",
      "33 0.77128\n",
      "34 0.7704\n",
      "34 0.769916\n",
      "34 0.769407\n",
      "34 0.768871\n",
      "34 0.77565\n",
      "35 0.774789\n",
      "35 0.774316\n",
      "35 0.773819\n",
      "35 0.773294\n",
      "35 0.779985\n",
      "36 0.779143\n",
      "36 0.778678\n",
      "36 0.778193\n",
      "36 0.77768\n",
      "36 0.78428\n",
      "37 0.783455\n",
      "37 0.782998\n",
      "37 0.782525\n",
      "37 0.782023\n",
      "37 0.788531\n",
      "38 0.787723\n",
      "38 0.787271\n",
      "38 0.786809\n",
      "38 0.786318\n",
      "38 0.792732\n",
      "39 0.79194\n",
      "39 0.791494\n",
      "39 0.791042\n",
      "39 0.790561\n",
      "39 0.796881\n",
      "40 0.796105\n",
      "40 0.795662\n",
      "40 0.795221\n",
      "40 0.794751\n",
      "40 0.800976\n",
      "41 0.800215\n",
      "41 0.799775\n",
      "41 0.799344\n",
      "41 0.798883\n",
      "41 0.805013\n",
      "42 0.804267\n",
      "42 0.803829\n",
      "42 0.803408\n",
      "42 0.802957\n",
      "42 0.808993\n",
      "43 0.80826\n",
      "43 0.807824\n",
      "43 0.807413\n",
      "43 0.806971\n",
      "43 0.812912\n",
      "44 0.812194\n",
      "44 0.811759\n",
      "44 0.811357\n",
      "44 0.810924\n",
      "44 0.816772\n",
      "45 0.816066\n",
      "45 0.815632\n",
      "45 0.815239\n",
      "45 0.814815\n",
      "45 0.82057\n",
      "46 0.819877\n",
      "46 0.819444\n",
      "46 0.81906\n",
      "46 0.818644\n",
      "46 0.824307\n",
      "47 0.823627\n",
      "47 0.823193\n",
      "47 0.822818\n",
      "47 0.82241\n",
      "47 0.827983\n",
      "48 0.827315\n",
      "48 0.826881\n",
      "48 0.826514\n",
      "48 0.826115\n",
      "48 0.831597\n",
      "49 0.830941\n",
      "49 0.830507\n",
      "49 0.830148\n",
      "49 0.829757\n",
      "49 0.835151\n",
      "50 0.834506\n",
      "50 0.834071\n",
      "50 0.83372\n",
      "50 0.833337\n",
      "50 0.838643\n",
      "51 0.838009\n",
      "51 0.837574\n",
      "51 0.837231\n",
      "51 0.836855\n",
      "51 0.842075\n",
      "52 0.841452\n",
      "52 0.841016\n",
      "52 0.840681\n",
      "52 0.840313\n",
      "52 0.845448\n",
      "53 0.844836\n",
      "53 0.844399\n",
      "53 0.844071\n",
      "53 0.84371\n",
      "53 0.848761\n",
      "54 0.848159\n",
      "54 0.847721\n",
      "54 0.847401\n",
      "54 0.847047\n",
      "54 0.852016\n",
      "55 0.851424\n",
      "55 0.850985\n",
      "55 0.850672\n",
      "55 0.850325\n",
      "55 0.855212\n",
      "56 0.854631\n",
      "56 0.854191\n",
      "56 0.853885\n",
      "56 0.853545\n",
      "56 0.858352\n",
      "57 0.85778\n",
      "57 0.857339\n",
      "57 0.85704\n",
      "57 0.856707\n",
      "57 0.861435\n",
      "58 0.860874\n",
      "58 0.860431\n",
      "58 0.860139\n",
      "58 0.859812\n",
      "58 0.864463\n",
      "59 0.863911\n",
      "59 0.863467\n",
      "59 0.863182\n",
      "59 0.862862\n",
      "59 0.867437\n",
      "60 0.866893\n",
      "60 0.866448\n",
      "60 0.86617\n",
      "60 0.865856\n",
      "60 0.870356\n",
      "61 0.869822\n",
      "61 0.869375\n",
      "61 0.869103\n",
      "61 0.868796\n",
      "61 0.873222\n",
      "62 0.872697\n",
      "62 0.872249\n",
      "62 0.871984\n",
      "62 0.871682\n",
      "62 0.876037\n",
      "63 0.87552\n",
      "63 0.875071\n",
      "63 0.874811\n",
      "63 0.874516\n",
      "63 0.8788\n",
      "64 0.878292\n",
      "64 0.877841\n",
      "64 0.877588\n",
      "64 0.877298\n",
      "64 0.881512\n",
      "65 0.881012\n",
      "65 0.880561\n",
      "65 0.880313\n",
      "65 0.880029\n",
      "65 0.884175\n",
      "66 0.883684\n",
      "66 0.883231\n",
      "66 0.882989\n",
      "66 0.882711\n",
      "66 0.886789\n",
      "67 0.886306\n",
      "67 0.885852\n",
      "67 0.885616\n",
      "67 0.885343\n",
      "67 0.889356\n",
      "68 0.88888\n",
      "68 0.888424\n",
      "68 0.888194\n",
      "68 0.887927\n",
      "68 0.891875\n",
      "69 0.891407\n",
      "69 0.89095\n",
      "69 0.890725\n",
      "69 0.890463\n",
      "69 0.894348\n",
      "70 0.893887\n",
      "70 0.893429\n",
      "70 0.89321\n",
      "70 0.892953\n",
      "70 0.896775\n",
      "71 0.896322\n",
      "71 0.895863\n",
      "71 0.895649\n",
      "71 0.895397\n",
      "71 0.899158\n",
      "72 0.898712\n",
      "72 0.898252\n",
      "72 0.898043\n",
      "72 0.897797\n",
      "72 0.901497\n",
      "73 0.901058\n",
      "73 0.900597\n",
      "73 0.900393\n",
      "73 0.900152\n",
      "73 0.903793\n",
      "74 0.903361\n",
      "74 0.902899\n",
      "74 0.9027\n",
      "74 0.902463\n",
      "74 0.906046\n",
      "75 0.905622\n",
      "75 0.905158\n",
      "75 0.904964\n",
      "75 0.904733\n",
      "75 0.908259\n",
      "76 0.907841\n",
      "76 0.907376\n",
      "76 0.907187\n",
      "76 0.90696\n",
      "76 0.91043\n",
      "77 0.910019\n",
      "77 0.909553\n",
      "77 0.909369\n",
      "77 0.909146\n",
      "77 0.912561\n",
      "78 0.912157\n",
      "78 0.91169\n",
      "78 0.91151\n",
      "78 0.911292\n",
      "78 0.914653\n",
      "79 0.914255\n",
      "79 0.913787\n",
      "79 0.913612\n",
      "79 0.913399\n",
      "79 0.916707\n",
      "80 0.916315\n",
      "80 0.915846\n",
      "80 0.915675\n",
      "80 0.915466\n",
      "80 0.918723\n",
      "81 0.918337\n",
      "81 0.917866\n",
      "81 0.917701\n",
      "81 0.917496\n",
      "81 0.920701\n",
      "82 0.920322\n",
      "82 0.91985\n",
      "82 0.919688\n",
      "82 0.919488\n",
      "82 0.922643\n",
      "83 0.922269\n",
      "83 0.921797\n",
      "83 0.921639\n",
      "83 0.921443\n",
      "83 0.924549\n",
      "84 0.924181\n",
      "84 0.923708\n",
      "84 0.923554\n",
      "84 0.923362\n",
      "84 0.92642\n",
      "85 0.926058\n",
      "85 0.925583\n",
      "85 0.925434\n",
      "85 0.925246\n",
      "85 0.928256\n",
      "86 0.9279\n",
      "86 0.927424\n",
      "86 0.927279\n",
      "86 0.927095\n",
      "86 0.930059\n",
      "87 0.929708\n",
      "87 0.929231\n",
      "87 0.92909\n",
      "87 0.928909\n",
      "87 0.931828\n",
      "88 0.931483\n",
      "88 0.931005\n",
      "88 0.930868\n",
      "88 0.930691\n",
      "88 0.933565\n",
      "89 0.933224\n",
      "89 0.932746\n",
      "89 0.932613\n",
      "89 0.932439\n",
      "89 0.935269\n",
      "90 0.934934\n",
      "90 0.934455\n",
      "90 0.934325\n",
      "90 0.934155\n",
      "90 0.936942\n",
      "91 0.936612\n",
      "91 0.936132\n",
      "91 0.936006\n",
      "91 0.93584\n",
      "91 0.938584\n",
      "92 0.93826\n",
      "92 0.937778\n",
      "92 0.937656\n",
      "92 0.937493\n",
      "92 0.940196\n",
      "93 0.939876\n",
      "93 0.939394\n",
      "93 0.939276\n",
      "93 0.939116\n",
      "93 0.941778\n",
      "94 0.941463\n",
      "94 0.94098\n",
      "94 0.940865\n",
      "94 0.940709\n",
      "94 0.943331\n",
      "95 0.943021\n",
      "95 0.942537\n",
      "95 0.942425\n",
      "95 0.942272\n",
      "95 0.944855\n",
      "96 0.944549\n",
      "96 0.944065\n",
      "96 0.943956\n",
      "96 0.943807\n",
      "96 0.946351\n",
      "97 0.94605\n",
      "97 0.945565\n",
      "97 0.94546\n",
      "97 0.945313\n",
      "97 0.947819\n",
      "98 0.947523\n",
      "98 0.947037\n",
      "98 0.946935\n",
      "98 0.946791\n",
      "98 0.949261\n",
      "99 0.948969\n",
      "99 0.948482\n",
      "99 0.948383\n",
      "99 0.948243\n",
      "99 0.950675\n",
      "0.950675\n",
      "0.0696034\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize variables (in this case just variable W)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(100):\n",
    "        for (x, y) in zip(trX, trY):\n",
    "            sess.run(train_op, feed_dict={X: x, Y: y})\n",
    "            print(i,sess.run(w))\n",
    "\n",
    "    print(sess.run(w))  # It should be something around 2\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
